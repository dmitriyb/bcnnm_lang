{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brian2 import *\n",
    "%matplotlib inline\n",
    "\n",
    "start_scope()\n",
    "\n",
    "#basic neuronal group characteristics\n",
    "neurons_num = 12 #referring to the number of stuctures and subgroups sizes\n",
    "\n",
    "v_rest = -70*mV #resting potential\n",
    "v_th = -50*mV #threshold potential\n",
    "area = 0.785*umetre**2 \n",
    "Cm = 1*ufarad*cm**-2 * area\n",
    "v_eK = -90*mV\n",
    "v_eNa = 50*mV\n",
    "g_Na = 100*msiemens*cm**-2 * area\n",
    "g_K = 30*msiemens*cm**-2 * area\n",
    "VT = -63*mV\n",
    "\n",
    "\n",
    "#Hodgkin-Huxley model equations\n",
    "eqs = '''\n",
    "dv/dt = -(g_Na*(m*m*m)*h*(v-v_eNa) + g_K*(n*n*n*n)*(v-v_eK) + I_stim)/Cm : volt\n",
    "dm/dt = 0.32*(mV**-1)*4*mV/exprel((13.*mV-v+VT)/(4*mV))/ms*(1-m)-0.28*(mV**-1)*5*mV/exprel((v-VT-40.*mV)/(5*mV))/ms*m : 1\n",
    "dn/dt = 0.032*(mV**-1)*5*mV/exprel((15.*mV-v+VT)/(5*mV))/ms*(1.-n)-.5*exp((10.*mV-v+VT)/(40.*mV))/ms*n : 1\n",
    "dh/dt = 0.128*exp((17.*mV-v+VT)/(18.*mV))/ms*(1.-h)-4./(1+exp((40.*mV-v+VT)/(5.*mV)))/ms*h : 1\n",
    "receptors_num : 1\n",
    "v_e : volt\n",
    "W : metre**3\n",
    "Kd : mM\n",
    "gam : siemens\n",
    "L : mM\n",
    "dI_stim/dt = (receptors_num*(v - v_e)*gam*((L/W/Kd)/(1*metre**-3 + (L/W/Kd))))/ms : amp\n",
    "\n",
    "'''\n",
    "\n",
    "model_glu = '''\n",
    "receptors_num_syn : 1\n",
    "v_e_syn : volt\n",
    "W_syn : metre**3\n",
    "Kd_syn : mM\n",
    "gam_syn : siemens\n",
    "L_syn : mM\n",
    "dI_stim_syn/dt = (receptors_num*(v - v_e)*gam*((L/W/Kd)/(1*metre**-3 + (L/W/Kd))))/ms : amp (event-driven)\n",
    "\n",
    "'''\n",
    "\n",
    "Prototype = NeuronGroup(neurons_num, eqs, threshold='v > v_th', reset='v = v_rest')\n",
    "Prototype.v = v_rest\n",
    "Prototype.receptors_num = 2400\n",
    "Prototype.v_e = +55*mV\n",
    "Prototype.gam = 10*psiemens\n",
    "Prototype.W = 20*nmeter * area\n",
    "Prototype.Kd = 500*nM\n",
    "Prototype.L = 1.1*mM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up test synapse\n",
    "#excitatory synapse (glutamate)\n",
    "S_012 = Synapses(Prototype, Prototype, model = Equations(model_glu))\n",
    "S_012.connect(i=[0], j=[1, 2])\n",
    "S_012.receptors_num_syn = 2400\n",
    "S_012.v_e_syn = +55*mV\n",
    "S_012.gam_syn = 10*psiemens\n",
    "S_012.W_syn = 20*nmeter * area\n",
    "S_012.Kd_syn = 500*nM\n",
    "S_012.L_syn = 1.1*mM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO       No numerical integration method specified for group 'neurongroup_1', using method 'euler' (took 0.05s, trying other methods took 0.26s). [brian2.stateupdaters.base.method_choice]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "M = StateMonitor(Prototype, 'v', record=True)\n",
    "\n",
    "run(5000*ms)\n",
    "\n",
    "fig, axs = plt.subplots(4)\n",
    "axs[0].plot(M.t/ms, M.v[0], color='blue', label='')\n",
    "axs[1].plot(M.t/ms, M.v[1], color='green', label='')\n",
    "axs[2].plot(M.t/ms, M.v[2], color='red', label='Th')\n",
    "axs[3].plot(M.t/ms, M.v[4], color='black', label='Th')\n",
    "plot()\n",
    "\n",
    "xlabel('Time (ms)')\n",
    "ylabel('v')\n",
    "legend();\n",
    "\n",
    "#set the spacing between subplots\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Документация по классу \"Синапс\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Module providing the `Synapses` class and related helper classes/functions.\n",
    "'''\n",
    "from collections import defaultdict\n",
    "from collections.abc import Sequence, MutableMapping, Mapping\n",
    "import functools\n",
    "import weakref\n",
    "import re\n",
    "import numbers\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from brian2.core.base import weakproxy_with_fallback\n",
    "from brian2.core.base import device_override\n",
    "from brian2.core.namespace import get_local_namespace\n",
    "from brian2.core.variables import (DynamicArrayVariable, Variables)\n",
    "from brian2.codegen.codeobject import create_runner_codeobj\n",
    "from brian2.codegen.translation import get_identifiers_recursively\n",
    "from brian2.devices.device import get_device\n",
    "from brian2.equations.equations import (Equations, SingleEquation,\n",
    "                                        DIFFERENTIAL_EQUATION, SUBEXPRESSION,\n",
    "                                        PARAMETER, INTEGER,\n",
    "                                        check_subexpressions)\n",
    "from brian2.groups.group import Group, CodeRunner, get_dtype\n",
    "from brian2.groups.neurongroup import (extract_constant_subexpressions,\n",
    "                                       SubexpressionUpdater,\n",
    "                                       check_identifier_pre_post)\n",
    "from brian2.parsing.expressions import is_boolean_expression, parse_expression_dimensions\n",
    "from brian2.stateupdaters.base import (StateUpdateMethod,\n",
    "                                       UnsupportedEquationsException)\n",
    "from brian2.stateupdaters.exact import linear, independent\n",
    "from brian2.units.fundamentalunits import (Quantity, DIMENSIONLESS, DimensionMismatchError,\n",
    "                                           fail_for_dimension_mismatch)\n",
    "from brian2.units.allunits import second\n",
    "from brian2.utils.logger import get_logger\n",
    "from brian2.utils.stringtools import get_identifiers, word_substitute\n",
    "from brian2.utils.arrays import calc_repeats\n",
    "from brian2.core.spikesource import SpikeSource\n",
    "from brian2.synapses.parse_synaptic_generator_syntax import parse_synapse_generator\n",
    "from brian2.parsing.bast import brian_ast\n",
    "from brian2.parsing.rendering import NodeRenderer\n",
    "\n",
    "MAX_SYNAPSES = 2147483647\n",
    "\n",
    "__all__ = ['Synapses']\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "[docs]class StateUpdater(CodeRunner):\n",
    "    '''\n",
    "    The `CodeRunner` that updates the state variables of a `Synapses`\n",
    "    at every timestep.\n",
    "    '''\n",
    "    def __init__(self, group, method, clock, order, method_options=None):\n",
    "        self.method_choice = method\n",
    "        self.method_options = method_options\n",
    "        CodeRunner.__init__(self, group,\n",
    "                            'stateupdate',\n",
    "                            clock=clock,\n",
    "                            when='groups',\n",
    "                            order=order,\n",
    "                            name=group.name + '_stateupdater',\n",
    "                            check_units=False,\n",
    "                            generate_empty_code=False)\n",
    "    \n",
    "[docs]    def update_abstract_code(self, run_namespace):\n",
    "        if len(self.group.equations) > 0:\n",
    "            # Resolve variables in the equations to correctly perform checks\n",
    "            # for repeated stateful functions (e.g. rand() calls)\n",
    "            names = self.group.equations.names\n",
    "            external_names = self.group.equations.identifiers | {'dt'}\n",
    "\n",
    "            variables = self.group.resolve_all(names | external_names,\n",
    "                                               run_namespace,\n",
    "                # we don't need to raise any warnings\n",
    "                # for the user here, warnings will\n",
    "                # be raised in create_runner_codeobj\n",
    "                user_identifiers=set())\n",
    "            stateupdate_output = StateUpdateMethod.apply_stateupdater(self.group.equations,\n",
    "                                                                      variables,\n",
    "                                                                      self.method_choice,\n",
    "                                                                      method_options=self.method_options,\n",
    "                                                                      group_name=self.group.name)\n",
    "            if isinstance(stateupdate_output, str):\n",
    "                self.abstract_code = stateupdate_output\n",
    "            else:\n",
    "                # Note that the reason to send self along with this method is so the StateUpdater\n",
    "                # can be modified! i.e. in GSL StateUpdateMethod a custom CodeObject gets added\n",
    "                # to the StateUpdater together with some auxiliary information\n",
    "                self.abstract_code = stateupdate_output(self)\n",
    "        else:\n",
    "            self.abstract_code = ''\n",
    "\n",
    "\n",
    "\n",
    "[docs]class SummedVariableUpdater(CodeRunner):\n",
    "    '''\n",
    "    The `CodeRunner` that updates a value in the target group with the\n",
    "    sum over values in the `Synapses` object.\n",
    "    '''\n",
    "    def __init__(self, expression, target_varname, synapses, target,\n",
    "                 target_size_name, index_var):\n",
    "        # Handling sumped variables using the standard mechanisms is not\n",
    "        # possible, we therefore also directly give the names of the arrays\n",
    "        # to the template.\n",
    "\n",
    "        code = '''\n",
    "        _synaptic_var = {expression}\n",
    "        '''.format(expression=expression,\n",
    "                   target_varname=target_varname)\n",
    "        self.target_varname = target_varname\n",
    "        self.expression = expression\n",
    "        self.target_var = synapses.variables[target_varname]\n",
    "        self.target = target\n",
    "        template_kwds = {'_target_var': self.target_var,\n",
    "                         '_target_size_name': target_size_name,\n",
    "                         '_index_var': synapses.variables[index_var],\n",
    "                         '_target_start': getattr(target, 'start', 0),\n",
    "                         '_target_stop': getattr(target, 'stop', -1)}\n",
    "\n",
    "        CodeRunner.__init__(self, group=synapses,\n",
    "                            template='summed_variable',\n",
    "                            code=code,\n",
    "                            needed_variables=[target_varname, target_size_name,\n",
    "                                              index_var],\n",
    "                            # We want to update the summed variable before\n",
    "                            # the target group gets updated\n",
    "                            clock=target.clock,\n",
    "                            when='groups',\n",
    "                            order=target.order-1,\n",
    "                            name=synapses.name + '_summed_variable_' + target_varname,\n",
    "                            template_kwds=template_kwds)\n",
    "\n",
    "\n",
    "\n",
    "[docs]class SynapticPathway(CodeRunner, Group):\n",
    "    '''\n",
    "    The `CodeRunner` that applies the pre/post statement(s) to the state\n",
    "    variables of synapses where the pre-/postsynaptic group spiked in this\n",
    "    time step.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    synapses : `Synapses`\n",
    "        Reference to the main `Synapses` object\n",
    "    prepost : {'pre', 'post'}\n",
    "        Whether this object should react to pre- or postsynaptic spikes\n",
    "    objname : str, optional\n",
    "        The name to use for the object, will be appendend to the name of\n",
    "        `synapses` to create a name in the sense of `Nameable`. If ``None``\n",
    "        is provided (the default), ``prepost`` will be used.\n",
    "    delay : `Quantity`, optional\n",
    "        A scalar delay (same delay for all synapses) for this pathway. If\n",
    "        not given, delays are expected to vary between synapses.\n",
    "    '''\n",
    "    def __init__(self, synapses, code, prepost, objname=None,\n",
    "                 delay=None, event='spike'):\n",
    "        self.code = code\n",
    "        self.prepost = prepost\n",
    "        self.event = event\n",
    "        if prepost == 'pre':\n",
    "            self.source = synapses.source\n",
    "            self.target = synapses.target\n",
    "            self.synapse_sources = synapses.variables['_synaptic_pre']\n",
    "            self.synapse_targets = synapses.variables['_synaptic_post']\n",
    "            order = -1\n",
    "        elif prepost == 'post':\n",
    "            self.source = synapses.target\n",
    "            self.target = synapses.source\n",
    "            self.synapse_sources = synapses.variables['_synaptic_post']\n",
    "            self.synapse_targets = synapses.variables['_synaptic_pre']\n",
    "            order = 1\n",
    "        else:\n",
    "            raise ValueError('prepost argument has to be either \"pre\" or '\n",
    "                             '\"post\"')\n",
    "        self.synapses = weakref.proxy(synapses)\n",
    "        # Allow to use the same indexing of the delay variable  as in the parent\n",
    "        # Synapses object (e.g. 2d indexing with pre- and post-synaptic indices)\n",
    "        self._indices = self.synapses._indices\n",
    "\n",
    "        if objname is None:\n",
    "            objname = prepost\n",
    "\n",
    "        CodeRunner.__init__(self, synapses,\n",
    "                            'synapses',\n",
    "                            code=code,\n",
    "                            clock=self.source.clock,\n",
    "                            when='synapses',\n",
    "                            order=order,\n",
    "                            name=synapses.name + '_' + objname,\n",
    "                            template_kwds={'pathway': self})\n",
    "\n",
    "        self._pushspikes_codeobj = None\n",
    "\n",
    "        self.spikes_start = self.source.start\n",
    "        self.spikes_stop = self.source.stop\n",
    "        self.eventspace_name = '_{}space'.format(event)\n",
    "        self.eventspace = None  # will be set in before_run\n",
    "        # Setting the Synapses object instead of \"self\" as an owner makes\n",
    "        # indexing conflicts disappear (e.g. with synapses connecting subgroups)\n",
    "        self.variables = Variables(synapses)\n",
    "        self.variables.add_reference(self.eventspace_name, self.source)\n",
    "        self.variables.add_reference('N', synapses)\n",
    "        if prepost == 'pre':\n",
    "            self.variables.add_reference('_n_sources', synapses, 'N_pre')\n",
    "            self.variables.add_reference('_n_targets', synapses, 'N_post')\n",
    "            self.variables.add_reference('_source_dt', synapses.source, 'dt')\n",
    "        else:\n",
    "            self.variables.add_reference('_n_sources', synapses, 'N_post')\n",
    "            self.variables.add_reference('_n_targets', synapses, 'N_pre')\n",
    "            self.variables.add_reference('_source_dt', synapses.target, 'dt')\n",
    "        if delay is None:  # variable delays\n",
    "            if getattr(synapses, 'N', None) is not None:\n",
    "                n_synapses = synapses.N\n",
    "            else:\n",
    "                n_synapses = 0\n",
    "            self.variables.add_dynamic_array('delay', dimensions=second.dim,\n",
    "                                             size=n_synapses, constant=True)\n",
    "            # Register the object with the `SynapticIndex` object so it gets\n",
    "            # automatically resized\n",
    "            synapses.register_variable(self.variables['delay'])\n",
    "        else:\n",
    "            if not isinstance(delay, Quantity):\n",
    "                raise TypeError(('Cannot set the delay for pathway \"%s\": '\n",
    "                                 'expected a quantity, got %s instead.') % (objname,\n",
    "                                                                            type(delay)))\n",
    "            if delay.size != 1:\n",
    "                raise TypeError(('Cannot set the delay for pathway \"%s\": '\n",
    "                                 'expected a scalar quantity, got a '\n",
    "                                 'quantity with shape %s instead.') % str(delay.shape))\n",
    "            fail_for_dimension_mismatch(delay, second, ('Delay has to be '\n",
    "                                                        'specified in units '\n",
    "                                                        'of seconds but got '\n",
    "                                                        '{value}'),\n",
    "                                        value=delay)\n",
    "            # We use a \"dynamic\" array of constant size here because it makes\n",
    "            # the generated code easier, we don't need to deal with a different\n",
    "            # type for scalar and variable delays\n",
    "            self.variables.add_dynamic_array('delay', dimensions=second.dim,\n",
    "                                             size=1, constant=True,\n",
    "                                             scalar=True)\n",
    "            # Since this array does not grow with the number of synapses, we\n",
    "            # have to resize it ourselves\n",
    "            self.variables['delay'].resize(1)\n",
    "            self.variables['delay'].set_value(delay)\n",
    "\n",
    "        self._delays = self.variables['delay']\n",
    "\n",
    "        # Re-extract the last part of the name from the full name\n",
    "        self.objname = self.name[len(synapses.name) + 1:]\n",
    "\n",
    "\n",
    "        #: The `CodeObject` initalising the `SpikeQueue` at the begin of a run\n",
    "        self._initialise_queue_codeobj = None\n",
    "\n",
    "        self.namespace = synapses.namespace\n",
    "\n",
    "        # Allow the use of string expressions referring to synaptic (including\n",
    "        # pre-/post-synaptic) variables\n",
    "        # Only include non-private variables (and their indices)\n",
    "        synaptic_vars = {varname for varname in list(synapses.variables)\n",
    "                         if not varname.startswith('_')}\n",
    "        synaptic_idcs = {varname: synapses.variables.indices[varname]\n",
    "                         for varname in synaptic_vars}\n",
    "        synaptic_vars |= {index_name for index_name in synaptic_idcs.values()\n",
    "                          if index_name not in ['_idx', '0']}\n",
    "        self.variables.add_references(synapses, synaptic_vars)\n",
    "        self.variables.indices.update(synaptic_idcs)\n",
    "\n",
    "        #: The `SpikeQueue`\n",
    "        self.queue = get_device().spike_queue(self.source.start, self.source.stop)\n",
    "        self.variables.add_object('_queue', self.queue)\n",
    "\n",
    "        self._enable_group_attributes()\n",
    "\n",
    "[docs]    def check_variable_write(self, variable):\n",
    "        # Forward the check to the `Synapses` object (raises an error if no\n",
    "        # synapse has been created yet)\n",
    "        self.synapses.check_variable_write(variable)\n",
    "\n",
    "\n",
    "    @device_override('synaptic_pathway_update_abstract_code')\n",
    "    def update_abstract_code(self, run_namespace=None, level=0):\n",
    "        if self.synapses.event_driven is not None:\n",
    "            event_driven_eqs = self.synapses.event_driven\n",
    "            clock_driven_eqs = self.synapses.equations\n",
    "            try:\n",
    "                event_driven_update = linear(event_driven_eqs,\n",
    "                                             self.group.variables)\n",
    "            except UnsupportedEquationsException:\n",
    "                # Check whether equations are independent\n",
    "                for var, expr in event_driven_eqs.diff_eq_expressions:\n",
    "                    for identifier in expr.identifiers:\n",
    "                        if identifier == var:\n",
    "                            continue\n",
    "                        if (identifier in event_driven_eqs.diff_eq_names or\n",
    "                                identifier in clock_driven_eqs):\n",
    "                            err = (\"Cannot solve the differential equation for \"\n",
    "                                   \"'{}' as event-driven, it depends on \"\n",
    "                                   \"another variable '{}'. Use (clock-driven) \"\n",
    "                                   \"instead.\".format(var,\n",
    "                                                                    identifier))\n",
    "                            raise UnsupportedEquationsException(err)\n",
    "                # All equations are independent, go ahead\n",
    "                event_driven_update = independent(self.synapses.event_driven,\n",
    "                                                  self.group.variables)\n",
    "            # TODO: Any way to do this more elegantly?\n",
    "            event_driven_update = re.sub(r'\\bdt\\b', '(t - lastupdate)',\n",
    "                                         event_driven_update)\n",
    "\n",
    "            self.abstract_code = event_driven_update + '\\n'\n",
    "        else:\n",
    "            self.abstract_code = ''\n",
    "\n",
    "        self.abstract_code += self.code + '\\n'\n",
    "        if self.synapses.event_driven is not None:\n",
    "            self.abstract_code += 'lastupdate = t\\n'\n",
    "\n",
    "    @device_override('synaptic_pathway_before_run')\n",
    "    def before_run(self, run_namespace):\n",
    "        super(SynapticPathway, self).before_run(run_namespace)\n",
    "\n",
    "[docs]    def create_code_objects(self, run_namespace):\n",
    "        if self._pushspikes_codeobj is None:\n",
    "            # Since this now works for general events not only spikes, we have to\n",
    "            # pass the information about which variable to use to the template,\n",
    "            # it can not longer simply refer to \"_spikespace\"\n",
    "            # Strictly speaking this is only true for the standalone mode at the\n",
    "            # moment, since in runtime, all the template does is to call\n",
    "            # SynapticPathway.push_spike\n",
    "            eventspace_name = '_{}space'.format(self.event)\n",
    "            template_kwds = {'eventspace_variable': self.source.variables[eventspace_name]}\n",
    "            needed_variables = [eventspace_name]\n",
    "            self._pushspikes_codeobj = create_runner_codeobj(self,\n",
    "                                                             '', # no code\n",
    "                                                             'synapses_push_spikes',\n",
    "                                                             name=self.name+'_push_spikes',\n",
    "                                                             check_units=False,\n",
    "                                                             additional_variables=self.variables,\n",
    "                                                             needed_variables=needed_variables,\n",
    "                                                             template_kwds=template_kwds,\n",
    "                                                             run_namespace=run_namespace)\n",
    "        self.code_objects[:] = [weakref.proxy(self._pushspikes_codeobj),\n",
    "                                weakref.proxy(self.create_default_code_object(run_namespace))]\n",
    "\n",
    "\n",
    "[docs]    def initialise_queue(self):\n",
    "        self.eventspace = self.source.variables[self.eventspace_name].get_value()\n",
    "        if not self.synapses._connect_called:\n",
    "            raise TypeError((\"Synapses object '%s' does not do anything, since \"\n",
    "                             \"it has not created synapses with 'connect'. \"\n",
    "                             \"Set its active attribute to False if you \"\n",
    "                             \"intend to do only do this for a subsequent\"\n",
    "                             \" run.\") % self.synapses.name)\n",
    "\n",
    "        # Update the dt (might have changed between runs)\n",
    "        self.queue.prepare(self._delays.get_value(), self.source.clock.dt_,\n",
    "                           self.synapse_sources.get_value())\n",
    "\n",
    "        if len({self.source.clock.dt_, self.synapses.clock.dt_,\n",
    "                self.target.clock.dt_}) > 1:\n",
    "            logger.warn((\"Note that the synaptic pathway '{pathway}' will run on the \"\n",
    "                         \"clock of the group '{source}' using a dt of {dt}. Either \"\n",
    "                         \"the Synapses object '{synapses}' or the target '{target}' \"\n",
    "                         \"(or both) are using a different dt. This might lead to \"\n",
    "                         \"unexpected results. In particular, all delays will be rounded to \"\n",
    "                         \"multiples of {dt}. If in doubt, try to ensure that \"\n",
    "                         \"'{source}', '{synapses}', and '{target}' use the \"\n",
    "                         \"same dt.\").format(pathway=self.name,\n",
    "                                            source=self.source.name,\n",
    "                                            target=self.target.name,\n",
    "                                            dt=self.source.clock.dt,\n",
    "                                            synapses=self.synapses.name),\n",
    "                        'synapses_dt_mismatch', once=True)\n",
    "\n",
    "\n",
    "    def _full_state(self):\n",
    "        state = super(SynapticPathway, self)._full_state()\n",
    "        if self.queue is not None:\n",
    "            state['_spikequeue'] = self.queue._full_state()\n",
    "        else:\n",
    "            state['_spikequeue'] = None\n",
    "        return state\n",
    "\n",
    "    def _restore_from_full_state(self, state):\n",
    "        # We have to handle the SpikeQueue separately from the other state\n",
    "        # variables, so remove it from the state dictionary so that it does not\n",
    "        # get treated as a state variable by the standard mechanism in\n",
    "        # `VariableOwner`\n",
    "        queue_state = state.pop('_spikequeue')\n",
    "        super(SynapticPathway, self)._restore_from_full_state(state)\n",
    "        if self.queue is None:\n",
    "            self.queue = get_device().spike_queue(self.source.start, self.source.stop)\n",
    "        self.queue._restore_from_full_state(queue_state)\n",
    "        # Put the spike queue state back for future restore calls\n",
    "        state['_spikequeue'] = queue_state\n",
    "\n",
    "[docs]    def push_spikes(self):\n",
    "        # Push new events (e.g. spikes) into the queue\n",
    "        events = self.eventspace[:self.eventspace[len(self.eventspace)-1]]\n",
    "\n",
    "        if len(events):\n",
    "            self.queue.push(events)\n",
    "\n",
    "\n",
    "\n",
    "[docs]def slice_to_test(x):\n",
    "    '''\n",
    "    Returns a testing function corresponding to whether an index is in slice x.\n",
    "    x can also be an int.\n",
    "    '''\n",
    "    try:\n",
    "        x = int(x)\n",
    "        return lambda y: (y == x)\n",
    "    except TypeError:\n",
    "        pass\n",
    "\n",
    "    if isinstance(x, slice):\n",
    "        if isinstance(x, slice) and x == slice(None):\n",
    "            # No need for testing\n",
    "            return lambda y: np.repeat(True, len(y))\n",
    "        start, stop, step = x.start, x.stop, x.step\n",
    "\n",
    "        if start is None:\n",
    "            # No need to test for >= start\n",
    "            if step is None:\n",
    "                # Only have a stop value\n",
    "                return lambda y: (y < stop)\n",
    "            else:\n",
    "                # Stop and step\n",
    "                return lambda y: (y < stop) & ((y % step) == 0)\n",
    "        else:\n",
    "            # We need to test for >= start\n",
    "            if step is None:\n",
    "                if stop is None:\n",
    "                    # Only a start value\n",
    "                    return lambda y: (y >= start)\n",
    "                else:\n",
    "                    # Start and stop\n",
    "                    return lambda y: (y >= start) & (y < stop)\n",
    "            else:\n",
    "                if stop is None:\n",
    "                    # Start and step value\n",
    "                    return lambda y: (y >= start) & ((y-start)%step == 0)\n",
    "                else:\n",
    "                    # Start, step and stop\n",
    "                    return lambda y: (y >= start) & ((y-start)%step == 0) & (y < stop)\n",
    "    else:\n",
    "        raise TypeError('Expected int or slice, got {} instead'.format(type(x)))\n",
    "\n",
    "\n",
    "\n",
    "[docs]def find_synapses(index, synaptic_neuron):\n",
    "    try:\n",
    "        index = int(index)\n",
    "    except TypeError:\n",
    "        pass\n",
    "\n",
    "    if isinstance(index, (int, slice)):\n",
    "        test = slice_to_test(index)\n",
    "        found = test(synaptic_neuron)\n",
    "        synapses = np.flatnonzero(found)\n",
    "    else:\n",
    "        synapses = []\n",
    "        for neuron in index:\n",
    "            targets = np.flatnonzero(synaptic_neuron == neuron)\n",
    "            synapses.extend(targets)\n",
    "        synapses = np.array(synapses, dtype=np.int32)\n",
    "\n",
    "    return synapses\n",
    "\n",
    "\n",
    "\n",
    "[docs]class SynapticSubgroup(object):\n",
    "    '''\n",
    "    A simple subgroup of `Synapses` that can be used for indexing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    indices : `ndarray` of int\n",
    "        The synaptic indices represented by this subgroup.\n",
    "    synaptic_pre : `DynamicArrayVariable`\n",
    "        References to all pre-synaptic indices. Only used to throw an error\n",
    "        when new synapses where added after creating this object.\n",
    "    '''\n",
    "    def __init__(self, synapses, indices):\n",
    "        self.synapses = weakproxy_with_fallback(synapses)\n",
    "        self._stored_indices = indices\n",
    "        self._synaptic_pre = synapses.variables['_synaptic_pre']\n",
    "        self._source_N = self._synaptic_pre.size  # total number of synapses\n",
    "\n",
    "    def _indices(self, index_var='_idx'):\n",
    "        if index_var != '_idx':\n",
    "            raise AssertionError('Did not expect index %s here.' % index_var)\n",
    "        if len(self._synaptic_pre.get_value()) != self._source_N:\n",
    "            raise RuntimeError(('Synapses have been added/removed since this '\n",
    "                                'synaptic subgroup has been created'))\n",
    "        return self._stored_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._stored_indices)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<%s, storing %d indices of %s>' % (self.__class__.__name__,\n",
    "                                                   len(self._stored_indices),\n",
    "                                                   self.synapses.name)\n",
    "\n",
    "\n",
    "\n",
    "[docs]class SynapticIndexing(object):\n",
    "\n",
    "    def __init__(self, synapses):\n",
    "        self.synapses = weakref.proxy(synapses)\n",
    "        self.source = weakproxy_with_fallback(self.synapses.source)\n",
    "        self.target = weakproxy_with_fallback(self.synapses.target)\n",
    "        self.synaptic_pre = synapses.variables['_synaptic_pre']\n",
    "        self.synaptic_post = synapses.variables['_synaptic_post']\n",
    "        if synapses.multisynaptic_index is not None:\n",
    "            self.synapse_number = synapses.variables[synapses.multisynaptic_index]\n",
    "        else:\n",
    "            self.synapse_number = None\n",
    "\n",
    "[docs]    def __call__(self, index=None, index_var='_idx'):\n",
    "        '''\n",
    "        Returns synaptic indices for `index`, which can be a tuple of indices\n",
    "        (including arrays and slices), a single index or a string.\n",
    "\n",
    "        '''\n",
    "        if index is None or (isinstance(index, str) and index == 'True'):\n",
    "            index = slice(None)\n",
    "\n",
    "        if (not isinstance(index, (tuple, str)) and\n",
    "                (isinstance(index, (numbers.Integral, np.ndarray, slice,\n",
    "                                   Sequence))\n",
    "                 or hasattr(index, '_indices'))):\n",
    "            if hasattr(index, '_indices'):\n",
    "                final_indices = index._indices(index_var=index_var).astype(np.int32)\n",
    "            elif isinstance(index, slice):\n",
    "                start, stop, step = index.indices(len(self.synaptic_pre.get_value()))\n",
    "                final_indices = np.arange(start, stop, step, dtype=np.int32)\n",
    "            else:\n",
    "                final_indices = np.asarray(index)\n",
    "        elif isinstance(index, tuple):\n",
    "            if len(index) == 2:  # two indices (pre- and postsynaptic cell)\n",
    "                index = (index[0], index[1], slice(None))\n",
    "            elif len(index) > 3:\n",
    "                raise IndexError('Need 1, 2 or 3 indices, got %d.' % len(index))\n",
    "\n",
    "            I, J, K = index\n",
    "            # Convert to absolute indices (e.g. for subgroups)\n",
    "            # Allow the indexing to fail, we'll later return an empty array in\n",
    "            # that case\n",
    "            try:\n",
    "                if hasattr(I, '_indices'):  # will return absolute indices already\n",
    "                    I = I._indices()\n",
    "                else:\n",
    "                    I = self.source._indices(I)\n",
    "                pre_synapses = find_synapses(I, self.synaptic_pre.get_value())\n",
    "            except IndexError:\n",
    "                pre_synapses = np.array([], dtype=np.int32)\n",
    "            try:\n",
    "                if hasattr(J, '_indices'):\n",
    "                    J = J._indices()\n",
    "                else:\n",
    "                    J = self.target._indices(J)\n",
    "                post_synapses = find_synapses(J, self.synaptic_post.get_value())\n",
    "            except IndexError:\n",
    "                post_synapses = np.array([], dtype=np.int32)\n",
    "\n",
    "            matching_synapses = np.intersect1d(pre_synapses, post_synapses,\n",
    "                                               assume_unique=True)\n",
    "\n",
    "            if isinstance(K, slice) and K == slice(None):\n",
    "                final_indices = matching_synapses\n",
    "            else:\n",
    "                if self.synapse_number is None:\n",
    "                    raise IndexError('To index by the third dimension you need '\n",
    "                                     'to switch on the calculation of the '\n",
    "                                     '\"multisynaptic_index\" when you create '\n",
    "                                     'the Synapses object.')\n",
    "                if isinstance(K, (numbers.Integral, slice)):\n",
    "                    test_k = slice_to_test(K)\n",
    "                else:\n",
    "                    raise NotImplementedError(('Indexing synapses with arrays not'\n",
    "                                               'implemented yet'))\n",
    "\n",
    "                # We want to access the raw arrays here, not go through the Variable\n",
    "                pre_neurons = self.synaptic_pre.get_value()[matching_synapses]\n",
    "                post_neurons = self.synaptic_post.get_value()[matching_synapses]\n",
    "                synapse_numbers = self.synapse_number.get_value()[matching_synapses]\n",
    "                final_indices = np.intersect1d(matching_synapses,\n",
    "                                               np.flatnonzero(test_k(synapse_numbers)),\n",
    "                                               assume_unique=True)\n",
    "        else:\n",
    "            raise IndexError('Unsupported index type {itype}'.format(itype=type(index)))\n",
    "\n",
    "        if index_var not in ('_idx', '0'):\n",
    "            return index_var.get_value()[final_indices.astype(np.int32)]\n",
    "        else:\n",
    "            return final_indices.astype(np.int32)\n",
    "\n",
    "\n",
    "\n",
    "[docs]class Synapses(Group):\n",
    "    '''\n",
    "    Class representing synaptic connections.\n",
    "\n",
    "    Creating a new `Synapses` object does by default not create any synapses,\n",
    "    you have to call the `Synapses.connect` method for that.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    source : `SpikeSource`\n",
    "        The source of spikes, e.g. a `NeuronGroup`.\n",
    "    target : `Group`, optional\n",
    "        The target of the spikes, typically a `NeuronGroup`. If none is given,\n",
    "        the same as `source`\n",
    "    model : `str`, `Equations`, optional\n",
    "        The model equations for the synapses.\n",
    "    on_pre : str, dict, optional\n",
    "        The code that will be executed after every pre-synaptic spike. Can be\n",
    "        either a single (possibly multi-line) string, or a dictionary mapping\n",
    "        pathway names to code strings. In the first case, the pathway will be\n",
    "        called ``pre`` and made available as an attribute of the same name.\n",
    "        In the latter case, the given names will be used as the\n",
    "        pathway/attribute names. Each pathway has its own code and its own\n",
    "        delays.\n",
    "    pre : str, dict, optional\n",
    "        Deprecated. Use ``on_pre`` instead.\n",
    "    on_post : str, dict, optional\n",
    "        The code that will be executed after every post-synaptic spike. Same\n",
    "        conventions as for `on_pre``, the default name for the pathway is\n",
    "        ``post``.\n",
    "    post : str, dict, optional\n",
    "        Deprecated. Use ``on_post`` instead.\n",
    "    delay : `Quantity`, dict, optional\n",
    "        The delay for the \"pre\" pathway (same for all synapses) or a dictionary\n",
    "        mapping pathway names to delays. If a delay is specified in this way\n",
    "        for a pathway, it is stored as a single scalar value. It can still\n",
    "        be changed afterwards, but only to a single scalar value. If you want\n",
    "        to have delays that vary across synapses, do not use the keyword\n",
    "        argument, but instead set the delays via the attribute of the pathway,\n",
    "        e.g. ``S.pre.delay = ...`` (or ``S.delay = ...`` as an abbreviation),\n",
    "        ``S.post.delay = ...``, etc.\n",
    "    on_event : str or dict, optional\n",
    "        Define the events which trigger the pre and post pathways. By default,\n",
    "        both pathways are triggered by the ``'spike'`` event, i.e. the event\n",
    "        that is triggered by the ``threshold`` condition in the connected\n",
    "        groups.\n",
    "    multisynaptic_index : str, optional\n",
    "        The name of a variable (which will be automatically created) that stores\n",
    "        the \"synapse number\". This number enumerates all synapses between the\n",
    "        same source and target so that they can be distinguished. For models\n",
    "        where each source-target pair has only a single connection, this number\n",
    "        only wastes memory (it would always default to 0), it is therefore not\n",
    "        stored by default. Defaults to ``None`` (no variable).\n",
    "    namespace : dict, optional\n",
    "        A dictionary mapping identifier names to objects. If not given, the\n",
    "        namespace will be filled in at the time of the call of `Network.run`,\n",
    "        with either the values from the ``namespace`` argument of the\n",
    "        `Network.run` method or from the local context, if no such argument is\n",
    "        given.\n",
    "    dtype : `dtype`, dict, optional\n",
    "        The `numpy.dtype` that will be used to store the values, or a\n",
    "        dictionary specifying the type for variable names. If a value is not\n",
    "        provided for a variable (or no value is provided at all), the preference\n",
    "        setting `core.default_float_dtype` is used.\n",
    "    codeobj_class : class, optional\n",
    "        The `CodeObject` class to use to run code.\n",
    "    dt : `Quantity`, optional\n",
    "        The time step to be used for the update of the state variables.\n",
    "        Cannot be combined with the `clock` argument.\n",
    "    clock : `Clock`, optional\n",
    "        The update clock to be used. If neither a clock, nor the `dt` argument\n",
    "        is specified, the `defaultclock` will be used.\n",
    "    order : int, optional\n",
    "        The priority of of this group for operations occurring at the same time\n",
    "        step and in the same scheduling slot. Defaults to 0.\n",
    "    method : str, `StateUpdateMethod`, optional\n",
    "        The numerical integration method to use. If none is given, an\n",
    "        appropriate one is automatically determined.\n",
    "    name : str, optional\n",
    "        The name for this object. If none is given, a unique name of the form\n",
    "        ``synapses``, ``synapses_1``, etc. will be automatically chosen.\n",
    "    '''\n",
    "    add_to_magic_network = True\n",
    "\n",
    "    def __init__(self, source, target=None, model=None, on_pre=None,\n",
    "                 pre=None, on_post=None, post=None,\n",
    "                 connect=None, delay=None, on_event='spike',\n",
    "                 multisynaptic_index=None,\n",
    "                 namespace=None, dtype=None,\n",
    "                 codeobj_class=None,\n",
    "                 dt=None, clock=None, order=0,\n",
    "                 method=('exact', 'euler', 'heun'),\n",
    "                 method_options=None,\n",
    "                 name='synapses*'):\n",
    "        if connect is not None:\n",
    "            raise TypeError('The connect keyword argument is no longer '\n",
    "                            'supported, call the connect method instead.')\n",
    "\n",
    "        if pre is not None:\n",
    "            if on_pre is not None:\n",
    "                raise TypeError(\"Cannot specify both 'pre' and 'on_pre'. The \"\n",
    "                                \"'pre' keyword is deprecated, use the 'on_pre' \"\n",
    "                                \"keyword instead.\")\n",
    "            logger.warn(\"The 'pre' keyword is deprecated, use 'on_pre' \"\n",
    "                        \"instead.\", 'deprecated_pre', once=True)\n",
    "            on_pre = pre\n",
    "\n",
    "        if post is not None:\n",
    "            if on_post is not None:\n",
    "                raise TypeError(\"Cannot specify both 'post' and 'on_post'. The \"\n",
    "                                \"'post' keyword is deprecated, use the \"\n",
    "                                \"'on_post' keyword instead.\")\n",
    "            logger.warn(\"The 'post' keyword is deprecated, use 'on_post' \"\n",
    "                        \"instead.\", 'deprecated_post', once=True)\n",
    "            on_post = post\n",
    "\n",
    "        Group.__init__(self, dt=dt, clock=clock, when='start', order=order,\n",
    "                       name=name)\n",
    "\n",
    "        if dtype is None:\n",
    "            dtype = {}\n",
    "        if isinstance(dtype, MutableMapping):\n",
    "            dtype['lastupdate'] = self._clock.variables['t'].dtype\n",
    "\n",
    "        #: remember whether connect was called to raise an error if an\n",
    "        #: assignment to a synaptic variable is attempted without a preceding\n",
    "        #: connect.\n",
    "        self._connect_called = False\n",
    "        self.codeobj_class = codeobj_class\n",
    "\n",
    "        self.source = source\n",
    "        self.add_dependency(source)\n",
    "        if target is None:\n",
    "            self.target = self.source\n",
    "        else:\n",
    "            self.target = target\n",
    "            self.add_dependency(target)\n",
    "            \n",
    "        ##### Prepare and validate equations\n",
    "        if model is None:\n",
    "            model = ''\n",
    "\n",
    "        if isinstance(model, str):\n",
    "            model = Equations(model)\n",
    "        if not isinstance(model, Equations):\n",
    "            raise TypeError(('model has to be a string or an Equations '\n",
    "                             'object, is \"%s\" instead.') % type(model))\n",
    "\n",
    "        # Check flags\n",
    "        model.check_flags({DIFFERENTIAL_EQUATION: ['event-driven', 'clock-driven'],\n",
    "                           SUBEXPRESSION: ['summed', 'shared',\n",
    "                                           'constant over dt'],\n",
    "                           PARAMETER: ['constant', 'shared']},\n",
    "                          incompatible_flags=[('event-driven', 'clock-driven'),\n",
    "                                              # 'summed' cannot be combined with\n",
    "                                              # any other flag\n",
    "                                              ('summed', 'shared',\n",
    "                                               'constant over dt')])\n",
    "\n",
    "        for name in ['i', 'j', 'delay']:\n",
    "            if name in model.names:\n",
    "                raise SyntaxError('\"%s\" is a reserved name that cannot be '\n",
    "                                  'used as a variable name.' % name)\n",
    "\n",
    "        # Add the \"multisynaptic index\", if desired\n",
    "        self.multisynaptic_index = multisynaptic_index\n",
    "        if multisynaptic_index is not None:\n",
    "            if not isinstance(multisynaptic_index, str):\n",
    "                raise TypeError('multisynaptic_index argument has to be a string')\n",
    "            model = model + Equations('{} : integer'.format(multisynaptic_index))\n",
    "\n",
    "        # Separate subexpressions depending whether they are considered to be\n",
    "        # constant over a time step or not\n",
    "        model, constant_over_dt = extract_constant_subexpressions(model)\n",
    "\n",
    "        # Separate the equations into event-driven equations,\n",
    "        # continuously updated equations and summed variable updates\n",
    "        event_driven = []\n",
    "        continuous = []\n",
    "        summed_updates = []\n",
    "        for single_equation in model.values():\n",
    "            if 'event-driven' in single_equation.flags:\n",
    "                event_driven.append(single_equation)\n",
    "            elif 'summed' in single_equation.flags:\n",
    "                summed_updates.append(single_equation)\n",
    "            else:\n",
    "                if (single_equation.type == DIFFERENTIAL_EQUATION and\n",
    "                            'clock-driven' not in single_equation.flags):\n",
    "                    logger.info(('The synaptic equation for the variable {var} '\n",
    "                                 'does not specify whether it should be '\n",
    "                                 'integrated at every timestep (\"clock-driven\") '\n",
    "                                 'or only at spiking events (\"event-driven\"). '\n",
    "                                 'It will be integrated at every timestep '\n",
    "                                 'which can slow down your simulation '\n",
    "                                 'unnecessarily if you only need the values of '\n",
    "                                 'this variable whenever a spike occurs. '\n",
    "                                 'Specify the equation as clock-driven '\n",
    "                                 'explicitly to avoid this '\n",
    "                                 'warning.').format(var=single_equation.varname),\n",
    "                                'clock_driven',\n",
    "                                once=True)\n",
    "                continuous.append(single_equation)\n",
    "\n",
    "        if len(event_driven):\n",
    "            self.event_driven = Equations(event_driven)\n",
    "            # Add the lastupdate variable, needed for event-driven updates\n",
    "            model += Equations('lastupdate : second')\n",
    "        else:\n",
    "            self.event_driven = None\n",
    "\n",
    "        self._create_variables(model, user_dtype=dtype)\n",
    "        self.equations = Equations(continuous)\n",
    "\n",
    "        if namespace is None:\n",
    "            namespace = {}\n",
    "        #: The group-specific namespace\n",
    "        self.namespace = namespace\n",
    "\n",
    "        #: Set of `Variable` objects that should be resized when the\n",
    "        #: number of synapses changes\n",
    "        self._registered_variables = set()\n",
    "\n",
    "        for varname, var in self.variables.items():\n",
    "            if (isinstance(var, DynamicArrayVariable) and\n",
    "                        self.variables.indices[varname] == '_idx'):\n",
    "                # Register the array with the `SynapticItemMapping` object so\n",
    "                # it gets automatically resized\n",
    "                self.register_variable(var)\n",
    "\n",
    "        # Support 2d indexing\n",
    "        self._indices = SynapticIndexing(self)\n",
    "\n",
    "        if delay is None:\n",
    "            delay = {}\n",
    "\n",
    "        if isinstance(delay, Quantity):\n",
    "            delay = {'pre': delay}\n",
    "        elif not isinstance(delay, Mapping):\n",
    "            raise TypeError('Delay argument has to be a quantity or a '\n",
    "                            'dictionary, is type %s instead.' % type(delay))\n",
    "\n",
    "        #: List of names of all updaters, e.g. ['pre', 'post']\n",
    "        self._synaptic_updaters = []\n",
    "        #: List of all `SynapticPathway` objects\n",
    "        self._pathways = []\n",
    "\n",
    "        if isinstance(on_event, str):\n",
    "            events_dict = defaultdict(lambda: on_event)\n",
    "        else:\n",
    "            events_dict = defaultdict(lambda: 'spike')\n",
    "            events_dict.update(on_event)\n",
    "\n",
    "        #: \"Events\" for all the pathways\n",
    "        self.events = events_dict\n",
    "        for prepost, argument in zip(('pre', 'post'), (on_pre, on_post)):\n",
    "            if not argument:\n",
    "                continue\n",
    "            if isinstance(argument, str):\n",
    "                pathway_delay = delay.get(prepost, None)\n",
    "                self._add_updater(argument, prepost, delay=pathway_delay,\n",
    "                                  event=self.events[prepost])\n",
    "            elif isinstance(argument, Mapping):\n",
    "                for key, value in argument.items():\n",
    "                    if not isinstance(key, str):\n",
    "                        err_msg = ('Keys for the \"on_{}\" argument'\n",
    "                                   'have to be strings, got '\n",
    "                                   '{} instead.').format(prepost, type(key))\n",
    "                        raise TypeError(err_msg)\n",
    "                    pathway_delay = delay.get(key, None)\n",
    "                    self._add_updater(value, prepost, objname=key,\n",
    "                                      delay=pathway_delay, event=self.events[key])\n",
    "\n",
    "        # Check whether any delays were specified for pathways that don't exist\n",
    "        for pathway in delay:\n",
    "            if not pathway in self._synaptic_updaters:\n",
    "                raise ValueError(('Cannot set the delay for pathway '\n",
    "                                  '\"%s\": unknown pathway.') % pathway)\n",
    "\n",
    "        #: Performs numerical integration step\n",
    "        self.state_updater = None\n",
    "\n",
    "        # We only need a state update if we have differential equations\n",
    "        if len(self.equations.diff_eq_names):\n",
    "            self.state_updater = StateUpdater(self, method, method_options=method_options,\n",
    "                                              clock=self.clock,\n",
    "                                              order=order)\n",
    "            self.contained_objects.append(self.state_updater)\n",
    "\n",
    "        #: Update the \"constant over a time step\" subexpressions\n",
    "        self.subexpression_updater = None\n",
    "        if len(constant_over_dt) > 0:\n",
    "            self.subexpression_updater = SubexpressionUpdater(self,\n",
    "                                                              constant_over_dt)\n",
    "            self.contained_objects.append(self.subexpression_updater)\n",
    "\n",
    "        #: \"Summed variable\" mechanism -- sum over all synapses of a\n",
    "        #: pre-/postsynaptic target\n",
    "        self.summed_updaters = {}\n",
    "        # We want to raise an error if the same variable is updated twice\n",
    "        # using this mechanism. This could happen if the Synapses object\n",
    "        # connected a NeuronGroup to itself since then all variables are\n",
    "        # accessible as var_pre and var_post.\n",
    "        summed_targets = set()\n",
    "        for single_equation in summed_updates:\n",
    "            varname = single_equation.varname\n",
    "            if not (varname.endswith('_pre') or varname.endswith('_post')):\n",
    "                raise ValueError(('The summed variable \"%s\" does not end '\n",
    "                                  'in \"_pre\" or \"_post\".') % varname)\n",
    "            if not varname in self.variables:\n",
    "                raise ValueError(('The summed variable \"%s\" does not refer'\n",
    "                                  'to any known variable in the '\n",
    "                                  'target group.') % varname)\n",
    "            if varname.endswith('_pre'):\n",
    "                summed_target = self.source\n",
    "                summed_target_size_name = 'N_pre'\n",
    "                orig_varname = varname[:-4]\n",
    "                summed_var_index = '_synaptic_pre'\n",
    "            else:\n",
    "                summed_target = self.target\n",
    "                summed_target_size_name = 'N_post'\n",
    "                orig_varname = varname[:-5]\n",
    "                summed_var_index = '_synaptic_post'\n",
    "\n",
    "            target_eq = getattr(summed_target, 'equations', {}).get(orig_varname, None)\n",
    "            if target_eq is None or target_eq.type != PARAMETER:\n",
    "                raise ValueError(('The summed variable \"%s\" needs a '\n",
    "                                  'corresponding parameter \"%s\" in the '\n",
    "                                  'target group.') % (varname,\n",
    "                                                      orig_varname))\n",
    "\n",
    "            fail_for_dimension_mismatch(self.variables['_summed_'+varname].dim,\n",
    "                                        self.variables[varname].dim,\n",
    "                                        ('Summed variables need to have '\n",
    "                                         'the same units in Synapses '\n",
    "                                         'and the target group'))\n",
    "            if self.variables[varname] in summed_targets:\n",
    "                raise ValueError(('The target variable \"%s\" is already '\n",
    "                                  'updated by another summed '\n",
    "                                  'variable') % orig_varname)\n",
    "            summed_targets.add(self.variables[varname])\n",
    "            updater = SummedVariableUpdater(single_equation.expr,\n",
    "                                            varname, self, summed_target,\n",
    "                                            summed_target_size_name,\n",
    "                                            summed_var_index)\n",
    "            self.summed_updaters[varname] = updater\n",
    "            self.contained_objects.append(updater)\n",
    "\n",
    "        # Activate name attribute access\n",
    "        self._enable_group_attributes()\n",
    "\n",
    "    N_outgoing_pre = property(fget= lambda self: self.variables['N_outgoing'].get_value(),\n",
    "                              doc='The number of outgoing synapses for each neuron in the '\n",
    "                                  'pre-synaptic group.')\n",
    "    N_incoming_post = property(fget=lambda self: self.variables['N_incoming'].get_value(),\n",
    "                               doc='The number of incoming synapses for each neuron in the '\n",
    "                                   'post-synaptic group.')\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        indices = self.indices[item]\n",
    "        return SynapticSubgroup(self, indices)\n",
    "\n",
    "    def _set_delay(self, delay, with_unit):\n",
    "        if 'pre' not in self._synaptic_updaters:\n",
    "            raise AttributeError(\"Synapses do not have a 'pre' pathway, \"\n",
    "                                 \"do not know what 'delay' refers to.\")\n",
    "        # Note that we cannot simply say: \"self.pre.delay = delay\" because this\n",
    "        # would not correctly deal with references to external constants\n",
    "        var = self.pre.variables['delay']\n",
    "        if with_unit:\n",
    "            reference = var.get_addressable_value_with_unit('delay', self.pre)\n",
    "        else:\n",
    "            reference = var.get_addressable_value('delay', self.pre)\n",
    "        reference.set_item('True', delay, level=2)\n",
    "\n",
    "    def _get_delay(self, with_unit):\n",
    "        if 'pre' not in self._synaptic_updaters:\n",
    "            raise AttributeError(\"Synapses do not have a 'pre' pathway, \"\n",
    "                                 \"do not know what 'delay' refers to.\")\n",
    "        var = self.pre.variables['delay']\n",
    "        if with_unit:\n",
    "            return var.get_addressable_value_with_unit('delay', self.pre)\n",
    "        else:\n",
    "            return var.get_addressable_value('delay', self.pre)\n",
    "\n",
    "    delay = property(functools.partial(_get_delay, with_unit=True),\n",
    "                     functools.partial(_set_delay, with_unit=True),\n",
    "                     doc='The presynaptic delay (if a pre-synaptic pathway '\n",
    "                         'exists).')\n",
    "    delay_ = property(functools.partial(_get_delay, with_unit=False),\n",
    "                      functools.partial(_set_delay, with_unit=False),\n",
    "                      doc='The presynaptic delay without unit information (if a'\n",
    "                          'pre-synaptic pathway exists).')\n",
    "\n",
    "    def _add_updater(self, code, prepost, objname=None, delay=None,\n",
    "                     event='spike'):\n",
    "        '''\n",
    "        Add a new target updater. Users should call `add_pre` or `add_post`\n",
    "        instead.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        code : str\n",
    "            The abstract code that should be executed on pre-/postsynaptic\n",
    "            spikes.\n",
    "        prepost : {'pre', 'post'}\n",
    "            Whether the code is triggered by presynaptic or postsynaptic spikes\n",
    "        objname : str, optional\n",
    "            A name for the object, see `SynapticPathway` for more details.\n",
    "        delay : `Quantity`, optional\n",
    "            A scalar delay (same delay for all synapses) for this pathway. If\n",
    "            not given, delays are expected to vary between synapses.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        objname : str\n",
    "            The final name for the object. Equals `objname` if it was explicitly\n",
    "            given (and did not end in a wildcard character).\n",
    "\n",
    "        '''\n",
    "        if prepost == 'pre':\n",
    "            spike_group, group_name = self.source, 'Source'\n",
    "        elif prepost == 'post':\n",
    "            spike_group, group_name = self.target, 'Target'\n",
    "        else:\n",
    "            raise AssertionError(('\"prepost\" argument has to be \"pre\" or '\n",
    "                                  '\"post\", is \"%s\".') % prepost)\n",
    "        if event not in spike_group.events:\n",
    "            raise ValueError((\"%s group does not define an event \"\n",
    "                              \"'%s'.\") % (group_name, event))\n",
    "\n",
    "        if not isinstance(spike_group, SpikeSource) or not hasattr(spike_group, 'clock'):\n",
    "            raise TypeError(('%s has to be a SpikeSource with spikes and'\n",
    "                             ' clock attribute. Is type %r instead')\n",
    "                            % (group_name, type(spike_group)))\n",
    "\n",
    "        updater = SynapticPathway(self, code, prepost, objname,\n",
    "                                  delay=delay, event=event)\n",
    "        objname = updater.objname\n",
    "        if hasattr(self, objname):\n",
    "            raise ValueError(('Cannot add updater with name \"{name}\", synapses '\n",
    "                              'object already has an attribute with this '\n",
    "                              'name.').format(name=objname))\n",
    "\n",
    "        setattr(self, objname, updater)\n",
    "        self._synaptic_updaters.append(objname)\n",
    "        self._pathways.append(updater)\n",
    "        self.contained_objects.append(updater)\n",
    "        return objname\n",
    "\n",
    "    def _create_variables(self, equations, user_dtype=None):\n",
    "        '''\n",
    "        Create the variables dictionary for this `Synapses`, containing\n",
    "        entries for the equation variables and some standard entries.\n",
    "        '''\n",
    "        self.variables = Variables(self)\n",
    "\n",
    "        # Standard variables always present\n",
    "        self.variables.add_dynamic_array('_synaptic_pre', size=0,\n",
    "                                         dtype=np.int32, constant=True,\n",
    "                                         read_only=True)\n",
    "        self.variables.add_dynamic_array('_synaptic_post', size=0,\n",
    "                                         dtype=np.int32, constant=True,\n",
    "                                         read_only=True)\n",
    "        self.variables.create_clock_variables(self._clock)\n",
    "        if '_offset' in self.target.variables:\n",
    "            self.variables.add_reference('_target_offset', self.target,\n",
    "                                         '_offset')\n",
    "        else:\n",
    "            self.variables.add_constant('_target_offset', value=0)\n",
    "        if '_offset' in self.source.variables:\n",
    "            self.variables.add_reference('_source_offset', self.source,\n",
    "                                         '_offset')\n",
    "        else:\n",
    "            self.variables.add_constant('_source_offset', value=0)\n",
    "        # To cope with connections to/from other synapses, N_incoming/N_outgoing\n",
    "        # will be resized when synapses are created\n",
    "        self.variables.add_dynamic_array('N_incoming', size=0, dtype=np.int32,\n",
    "                                         constant=True,  read_only=True,\n",
    "                                         index='_postsynaptic_idx')\n",
    "        self.variables.add_dynamic_array('N_outgoing', size=0, dtype=np.int32,\n",
    "                                         constant=True,  read_only=True,\n",
    "                                         index='_presynaptic_idx')\n",
    "\n",
    "        # We have to make a distinction here between the indices\n",
    "        # and the arrays (even though they refer to the same object)\n",
    "        # the synaptic propagation template would otherwise overwrite\n",
    "        # synaptic_post in its namespace with the value of the\n",
    "        # postsynaptic index, leading to errors for the next\n",
    "        # propagation.\n",
    "        self.variables.add_reference('_presynaptic_idx',\n",
    "                                     self,\n",
    "                                     '_synaptic_pre')\n",
    "        self.variables.add_reference('_postsynaptic_idx',\n",
    "                                     self,\n",
    "                                     '_synaptic_post')\n",
    "\n",
    "        # Except for subgroups (which potentially add an offset), the \"i\" and\n",
    "        # \"j\" variables are simply equivalent to `_synaptic_pre` and\n",
    "        # `_synaptic_post`\n",
    "        if getattr(self.source, 'start', 0) == 0:\n",
    "            self.variables.add_reference('i', self, '_synaptic_pre')\n",
    "        else:\n",
    "            self.variables.add_reference('_source_i', self.source.source, 'i',\n",
    "                                         index='_presynaptic_idx')\n",
    "            self.variables.add_reference('_source_offset', self.source, '_offset')\n",
    "            self.variables.add_subexpression('i',\n",
    "                                             dtype=self.source.source.variables['i'].dtype,\n",
    "                                             expr='_source_i - _source_offset',\n",
    "                                             index='_presynaptic_idx')\n",
    "        if getattr(self.target, 'start', 0) == 0:\n",
    "            self.variables.add_reference('j', self, '_synaptic_post')\n",
    "        else:\n",
    "            self.variables.add_reference('_target_j', self.target.source, 'i',\n",
    "                                         index='_postsynaptic_idx')\n",
    "            self.variables.add_reference('_target_offset', self.target, '_offset')\n",
    "            self.variables.add_subexpression('j',\n",
    "                                             dtype=self.target.source.variables['i'].dtype,\n",
    "                                             expr='_target_j - _target_offset',\n",
    "                                             index='_postsynaptic_idx')\n",
    "\n",
    "        # Add the standard variables\n",
    "        self.variables.add_array('N',  dtype=np.int32, size=1, scalar=True,\n",
    "                                 constant=True, read_only=True)\n",
    "\n",
    "        for eq in equations.values():\n",
    "            dtype = get_dtype(eq, user_dtype)\n",
    "            if eq.type in (DIFFERENTIAL_EQUATION, PARAMETER):\n",
    "                check_identifier_pre_post(eq.varname)\n",
    "                constant = 'constant' in eq.flags\n",
    "                shared = 'shared' in eq.flags\n",
    "                if shared:\n",
    "                    self.variables.add_array(eq.varname, size=1,\n",
    "                                             dimensions=eq.dim,\n",
    "                                             dtype=dtype,\n",
    "                                             constant=constant,\n",
    "                                             scalar=True,\n",
    "                                             index='0')\n",
    "                else:\n",
    "                    self.variables.add_dynamic_array(eq.varname, size=0,\n",
    "                                                     dimensions=eq.dim,\n",
    "                                                     dtype=dtype,\n",
    "                                                     constant=constant)\n",
    "            elif eq.type == SUBEXPRESSION:\n",
    "                if 'summed' in eq.flags:\n",
    "                    # Give a special name to the subexpression for summed\n",
    "                    # variables to avoid confusion with the pre/postsynaptic\n",
    "                    # target variable\n",
    "                    varname = '_summed_'+eq.varname\n",
    "                else:\n",
    "                    check_identifier_pre_post(eq.varname)\n",
    "                    varname = eq.varname\n",
    "                self.variables.add_subexpression(varname, dimensions=eq.dim,\n",
    "                                                 expr=str(eq.expr),\n",
    "                                                 scalar='shared' in eq.flags,\n",
    "                                                 dtype=dtype)\n",
    "            else:\n",
    "                raise AssertionError('Unknown type of equation: ' + eq.eq_type)\n",
    "\n",
    "        # Stochastic variables\n",
    "        for xi in equations.stochastic_variables:\n",
    "            self.variables.add_auxiliary_variable(xi, dimensions=(second ** -0.5).dim)\n",
    "\n",
    "        # Add all the pre and post variables with _pre and _post suffixes\n",
    "        for name in getattr(self.source, 'variables', {}):\n",
    "            # Raise an error if a variable name is also used for a synaptic\n",
    "            # variable (we ignore 'lastupdate' to allow connections from another\n",
    "            # Synapses object)\n",
    "            if (name in equations.names and name != 'lastupdate' and\n",
    "                    'summed' not in equations[name].flags):\n",
    "                error_msg = ('The pre-synaptic variable {name} has the same '\n",
    "                             'name as a synaptic variable, rename the synaptic '\n",
    "                             'variable ').format(name=name)\n",
    "                if name+'_syn' not in self.variables:\n",
    "                    error_msg += (\"(for example to '{name}_syn') \".format(name=name))\n",
    "                error_msg += 'to avoid confusion'\n",
    "                raise ValueError(error_msg)\n",
    "            if name.startswith('_'):\n",
    "                continue  # Do not add internal variables\n",
    "            var = self.source.variables[name]\n",
    "            index = '0' if var.scalar else '_presynaptic_idx'\n",
    "            try:\n",
    "                self.variables.add_reference(name + '_pre', self.source, name,\n",
    "                                             index=index)\n",
    "            except TypeError:\n",
    "                logger.diagnostic(('Cannot include a reference to {var} in '\n",
    "                                   '{synapses}, {var} uses a non-standard '\n",
    "                                   'indexing in the pre-synaptic group '\n",
    "                                   '{source}.').format(var=name,\n",
    "                                                       synapses=self.name,\n",
    "                                                       source=self.source.name))\n",
    "        for name in getattr(self.target, 'variables', {}):\n",
    "            # Raise an error if a variable name is also used for a synaptic\n",
    "            # variable (we ignore 'lastupdate' to allow connections to another\n",
    "            # Synapses object)\n",
    "            if (name in equations.names and name != 'lastupdate' and\n",
    "                    'summed' not in equations[name].flags):\n",
    "                error_msg = (\"The post-synaptic variable '{name}' has the same \"\n",
    "                             \"name as a synaptic variable, rename the synaptic \"\n",
    "                             \"variable \").format(name=name)\n",
    "                if name+'_syn' not in self.variables:\n",
    "                    error_msg += (\"(for example to '{name}_syn') \".format(name=name))\n",
    "                error_msg += 'to avoid confusion'\n",
    "                raise ValueError(error_msg)\n",
    "            if name.startswith('_'):\n",
    "                continue  # Do not add internal variables\n",
    "            var = self.target.variables[name]\n",
    "            index = '0' if var.scalar else '_postsynaptic_idx'\n",
    "            try:\n",
    "                self.variables.add_reference(name + '_post', self.target, name,\n",
    "                                             index=index)\n",
    "                # Also add all the post variables without a suffix, but only if\n",
    "                # it does not have a post or pre suffix in the target group\n",
    "                # (which could happen when connecting to synapses)\n",
    "                if not name.endswith('_post') or name.endswith('_pre'):\n",
    "                    self.variables.add_reference(name, self.target, name,\n",
    "                                                 index=index)\n",
    "            except TypeError:\n",
    "                logger.diagnostic(('Cannot include a reference to {var} in '\n",
    "                                   '{synapses}, {var} uses a non-standard '\n",
    "                                   'indexing in the post-synaptic group '\n",
    "                                   '{target}.').format(var=name,\n",
    "                                                       synapses=self.name,\n",
    "                                                       target=self.target.name))\n",
    "\n",
    "        # Check scalar subexpressions\n",
    "        for eq in equations.values():\n",
    "            if eq.type == SUBEXPRESSION and 'shared' in eq.flags:\n",
    "                var = self.variables[eq.varname]\n",
    "                for identifier in var.identifiers:\n",
    "                    if identifier in self.variables:\n",
    "                        if not self.variables[identifier].scalar:\n",
    "                            raise SyntaxError(('Shared subexpression %s refers '\n",
    "                                               'to non-shared variable %s.')\n",
    "                                              % (eq.varname, identifier))\n",
    "\n",
    "[docs]    def before_run(self, run_namespace):\n",
    "        self.equations.check_units(self, run_namespace=run_namespace)\n",
    "        # Check that subexpressions that refer to stateful functions are labeled\n",
    "        # as \"constant over dt\"\n",
    "        check_subexpressions(self, self.equations, run_namespace)\n",
    "        super(Synapses, self).before_run(run_namespace=run_namespace)\n",
    "\n",
    "\n",
    "    @device_override('synapses_connect')\n",
    "    def connect(self, condition=None, i=None, j=None, p=1., n=1,\n",
    "                skip_if_invalid=False,\n",
    "                namespace=None, level=0):\n",
    "        '''\n",
    "        Add synapses.\n",
    "\n",
    "        See :doc:`/user/synapses` for details.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        condition : str, bool, optional\n",
    "            A boolean or string expression that evaluates to a boolean.\n",
    "            The expression can depend on indices ``i`` and ``j`` and on\n",
    "            pre- and post-synaptic variables. Can be combined with\n",
    "            arguments ``n``, and ``p`` but not ``i`` or ``j``.\n",
    "        i : int, ndarray of int, optional\n",
    "            The presynaptic neuron indices (in the form of an index or an array\n",
    "            of indices). Must be combined with ``j`` argument.\n",
    "        j : int, ndarray of int, str, optional\n",
    "            The postsynaptic neuron indices. It can be an index or array of\n",
    "            indices if combined with the ``i`` argument, or it can be a string\n",
    "            generator expression.\n",
    "        p : float, str, optional\n",
    "            The probability to create ``n`` synapses wherever the ``condition``\n",
    "            evaluates to true. Cannot be used with generator syntax for ``j``.\n",
    "        n : int, str, optional\n",
    "            The number of synapses to create per pre/post connection pair.\n",
    "            Defaults to 1.\n",
    "        skip_if_invalid : bool, optional\n",
    "            If set to True, rather than raising an error if you try to\n",
    "            create an invalid/out of range pair (i, j) it will just\n",
    "            quietly skip those synapses.\n",
    "        namespace : dict-like, optional\n",
    "            A namespace that will be used in addition to the group-specific\n",
    "            namespaces (if defined). If not specified, the locals\n",
    "            and globals around the run function will be used.\n",
    "        level : int, optional\n",
    "            How deep to go up the stack frame to look for the locals/global\n",
    "            (see ``namespace`` argument).\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        >>> from brian2 import *\n",
    "        >>> import numpy as np\n",
    "        >>> G = NeuronGroup(10, 'dv/dt = -v / tau : 1', threshold='v>1', reset='v=0')\n",
    "        >>> S = Synapses(G, G, 'w:1', on_pre='v+=w')\n",
    "        >>> S.connect(condition='i != j') # all-to-all but no self-connections\n",
    "        >>> S.connect(i=0, j=0) # connect neuron 0 to itself\n",
    "        >>> S.connect(i=np.array([1, 2]), j=np.array([2, 1])) # connect 1->2 and 2->1\n",
    "        >>> S.connect() # connect all-to-all\n",
    "        >>> S.connect(condition='i != j', p=0.1)  # Connect neurons with 10% probability, exclude self-connections\n",
    "        >>> S.connect(j='i', n=2)  # Connect all neurons to themselves with 2 synapses\n",
    "        >>> S.connect(j='k for k in range(i+1)') # Connect neuron i to all j with 0<=j<=i\n",
    "        >>> S.connect(j='i+(-1)**k for k in range(2) if i>0 and i<N_pre-1') # connect neuron i to its neighbours if it has both neighbours\n",
    "        >>> S.connect(j='k for k in sample(N_post, p=i*1.0/(N_pre-1))') # neuron i connects to j with probability i/(N-1)\n",
    "        '''\n",
    "        # check types\n",
    "        if condition is not None and not isinstance(condition, (bool,\n",
    "                                                                str)):\n",
    "            raise TypeError(\"condition argument must be bool or string. If you \"\n",
    "                            \"want to connect based on indices, use \"\n",
    "                            \"connect(i=..., j=...).\")\n",
    "        if i is not None and (not (isinstance(i, (numbers.Integral,\n",
    "                                                 np.ndarray,\n",
    "                                                 Sequence)) or\n",
    "                                   hasattr(i, '_indices')) or\n",
    "                              isinstance(i, str)):\n",
    "            raise TypeError(\"i argument must be int or array\")\n",
    "        if j is not None and not (isinstance(j, (numbers.Integral,\n",
    "                                                np.ndarray,\n",
    "                                                Sequence)) or\n",
    "                                  hasattr(j, '_indices')):\n",
    "            raise TypeError(\"j argument must be int, array or string\")\n",
    "        # TODO: eliminate these restrictions\n",
    "        if not isinstance(p, (int, float, str)):\n",
    "            raise TypeError(\"p must be float or string\")\n",
    "        if not isinstance(n, (int, str)):\n",
    "            raise TypeError(\"n must be int or string\")\n",
    "        if isinstance(condition, str) and re.search(r'\\bfor\\b',\n",
    "                                                           condition):\n",
    "            raise ValueError(\"Generator expression given for condition, write \"\n",
    "                             \"connect(j='{condition}'...) instead of \"\n",
    "                             \"connect('{condition}'...).\".format(condition=condition))\n",
    "\n",
    "        self._connect_called = True\n",
    "\n",
    "        # Get namespace information\n",
    "        if namespace is None:\n",
    "            namespace = get_local_namespace(level=level + 2)\n",
    "\n",
    "        # which connection case are we in?\n",
    "        if condition is None and i is None and j is None:\n",
    "            condition = True\n",
    "        try:\n",
    "            if condition is not None:\n",
    "                if i is not None or j is not None:\n",
    "                    raise ValueError(\"Cannot combine condition with i or j \"\n",
    "                                     \"arguments\")\n",
    "                # convert to generator syntax\n",
    "                if condition is False:\n",
    "                    return\n",
    "                if condition is True:\n",
    "                    condition = 'True'\n",
    "                # Check that the condition is a boolean expresion\n",
    "                identifiers = get_identifiers(condition)\n",
    "                variables = self.resolve_all(identifiers, namespace)\n",
    "                if not is_boolean_expression(condition, variables):\n",
    "                    raise TypeError(f'Condition \\'{condition}\\' is not a '\n",
    "                                    f'boolean condition')\n",
    "\n",
    "                # Check the units (mostly to check for unit consistency within the condition)\n",
    "                dims = parse_expression_dimensions(condition, variables)\n",
    "                if dims is not DIMENSIONLESS:\n",
    "                    # We should not get here normally\n",
    "                    raise TypeError(f'Condition \\'{condition}\\' is not a '\n",
    "                                    f'boolean condition')\n",
    "\n",
    "                condition = word_substitute(condition, {'j': '_k'})\n",
    "                if not isinstance(p, str) and p == 1:\n",
    "                    j = ('_k for _k in range(N_post) '\n",
    "                         'if {expr}').format(expr=condition)\n",
    "                else:\n",
    "                    j = None\n",
    "                    if isinstance(p, str):\n",
    "                        identifiers = get_identifiers(p)\n",
    "                        variables = self.resolve_all(identifiers, namespace)\n",
    "                        dim = parse_expression_dimensions(p, variables)\n",
    "                        if dim is not DIMENSIONLESS:\n",
    "                            raise DimensionMismatchError('Expression for p should be dimensionless.')\n",
    "                        p_dep = self._expression_index_dependence(p, namespace=namespace)\n",
    "                        if '_postsynaptic_idx' in p_dep or '_iterator_idx' in p_dep:\n",
    "                            j = ('_k for _k in range(N_post) '\n",
    "                                 'if ({expr}) and '\n",
    "                                 'rand()<{p}').format(expr=condition, p=p)\n",
    "                    if j is None:\n",
    "                        j = ('_k for _k in sample(N_post, p={p}) '\n",
    "                             'if {expr}').format(expr=condition, p=p)\n",
    "                # will now call standard generator syntax (see below)\n",
    "            elif i is not None:\n",
    "                if j is None:\n",
    "                    raise ValueError(\"i argument must be combined with j \"\n",
    "                                     \"argument\")\n",
    "                if skip_if_invalid:\n",
    "                    raise ValueError(\"Can only use skip_if_invalid with string \"\n",
    "                                     \"syntax\")\n",
    "                if hasattr(i, '_indices'):\n",
    "                    i = i._indices()\n",
    "                i = np.asarray(i)\n",
    "                if not np.issubdtype(i.dtype, np.signedinteger):\n",
    "                    raise TypeError(('Presynaptic indices have to be given as '\n",
    "                                     'integers, are type %s '\n",
    "                                     'instead.') % i.dtype)\n",
    "\n",
    "                if hasattr(j, '_indices'):\n",
    "                    j = j._indices()\n",
    "                j = np.asarray(j)\n",
    "                if not np.issubdtype(j.dtype, np.signedinteger):\n",
    "                    raise TypeError(('Presynaptic indices can only be combined '\n",
    "                                     'with postsynaptic integer indices))'))\n",
    "                if isinstance(n, str):\n",
    "                    raise TypeError(('Indices cannot be combined with a string'\n",
    "                                     'expression for n. Either use an '\n",
    "                                     'array/scalar for n, or a string '\n",
    "                                     'expression for the connections'))\n",
    "                i, j, n = np.broadcast_arrays(i, j, n)\n",
    "                if i.ndim > 1:\n",
    "                    raise ValueError('Can only use 1-dimensional indices')\n",
    "                self._add_synapses_from_arrays(i, j, n, p, namespace=namespace)\n",
    "                return\n",
    "            elif j is not None:\n",
    "                if isinstance(p, str) or p != 1:\n",
    "                    raise ValueError(\"Generator syntax cannot be combined with \"\n",
    "                                     \"p argument\")\n",
    "                if not re.search(r'\\bfor\\b', j):\n",
    "                    if_split = j.split(' if ')\n",
    "                    if len(if_split) == 1:\n",
    "                        j = '{j} for _ in range(1)'.format(j=j)\n",
    "                    elif len(if_split) == 2:\n",
    "                        j = '{target} for _ in range(1) if {cond}'.format(target=if_split[0],\n",
    "                                                                          cond=if_split[1])\n",
    "                    else:\n",
    "                        raise SyntaxError(\"Error parsing expression '{j}'. \"\n",
    "                                          \"Expression must have generator \"\n",
    "                                          \"syntax, for example 'k for k in \"\n",
    "                                          \"range(i-10, i+10)'\".format(j=j))\n",
    "                    # will now call standard generator syntax (see below)\n",
    "            else:\n",
    "                raise ValueError(\"Must specify at least one of condition, i or \"\n",
    "                                 \"j arguments\")\n",
    "\n",
    "            # standard generator syntax\n",
    "            self._add_synapses_generator(j, n, skip_if_invalid=skip_if_invalid,\n",
    "                                         namespace=namespace, level=level+2)\n",
    "        except IndexError as e:\n",
    "            raise IndexError(\"Tried to create synapse indices outside valid \"\n",
    "                             \"range. Original error message: \" + str(e))\n",
    "\n",
    "[docs]    def check_variable_write(self, variable):\n",
    "        '''\n",
    "        Checks that `Synapses.connect` has been called before setting a\n",
    "        synaptic variable.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        variable : `Variable`\n",
    "            The variable that the user attempts to set.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        TypeError\n",
    "            If `Synapses.connect` has not been called yet.\n",
    "        '''\n",
    "        if not self._connect_called:\n",
    "            raise TypeError((\"Cannot write to synaptic variable '%s', you need \"\n",
    "                             \"to call connect(...) first\") % variable.name)\n",
    "\n",
    "\n",
    "    def _resize(self, number):\n",
    "        if not isinstance(number, (numbers.Integral, np.integer)):\n",
    "            raise TypeError(('Expected an integer number got {} '\n",
    "                             'instead').format(type(number)))\n",
    "        if number < self.N:\n",
    "            raise ValueError(('Cannot reduce number of synapses, '\n",
    "                              '{} < {}').format(number, len(self)))\n",
    "\n",
    "        for variable in self._registered_variables:\n",
    "            variable.resize(number)\n",
    "\n",
    "        self.variables['N'].set_value(number)\n",
    "\n",
    "    def _update_synapse_numbers(self, old_num_synapses):\n",
    "        source_offset = self.variables['_source_offset'].get_value()\n",
    "        target_offset = self.variables['_target_offset'].get_value()\n",
    "        # This resizing is only necessary if we are connecting to/from synapses\n",
    "        post_with_offset = (int(self.variables['N_post'].get_value()) +\n",
    "                            target_offset)\n",
    "        pre_with_offset = (int(self.variables['N_pre'].get_value()) +\n",
    "                           source_offset)\n",
    "        self.variables['N_incoming'].resize(post_with_offset)\n",
    "        self.variables['N_outgoing'].resize(pre_with_offset)\n",
    "        N_outgoing = self.variables['N_outgoing'].get_value()\n",
    "        N_incoming = self.variables['N_incoming'].get_value()\n",
    "        synaptic_pre = self.variables['_synaptic_pre'].get_value()\n",
    "        synaptic_post = self.variables['_synaptic_post'].get_value()\n",
    "\n",
    "        # Update the number of total outgoing/incoming synapses per\n",
    "        # source/target neuron\n",
    "        N_outgoing[:] += np.bincount(synaptic_pre[old_num_synapses:],\n",
    "                                     minlength=len(N_outgoing))\n",
    "        N_incoming[:] += np.bincount(synaptic_post[old_num_synapses:],\n",
    "                                     minlength=len(N_incoming))\n",
    "\n",
    "        if self.multisynaptic_index is not None:\n",
    "            synapse_number_var = self.variables[self.multisynaptic_index]\n",
    "            synapse_number = synapse_number_var.get_value()\n",
    "\n",
    "            # Update the \"synapse number\" (number of synapses for the same\n",
    "            # source-target pair)\n",
    "            # We wrap pairs of source/target indices into a complex number for\n",
    "            # convenience\n",
    "            _source_target_pairs = synaptic_pre + synaptic_post*1j\n",
    "            synapse_number[:] = calc_repeats(_source_target_pairs)\n",
    "\n",
    "[docs]    def register_variable(self, variable):\n",
    "        '''\n",
    "        Register a `DynamicArray` to be automatically resized when the size of\n",
    "        the indices change. Called automatically when a `SynapticArrayVariable`\n",
    "        specifier is created.\n",
    "        '''\n",
    "        if not hasattr(variable, 'resize'):\n",
    "            raise TypeError(('Variable of type {} does not have a resize '\n",
    "                             'method, cannot register it with the synaptic '\n",
    "                             'indices.').format(type(variable)))\n",
    "        self._registered_variables.add(variable)\n",
    "\n",
    "\n",
    "[docs]    def unregister_variable(self, variable):\n",
    "        '''\n",
    "        Unregister a `DynamicArray` from the automatic resizing mechanism.\n",
    "        '''\n",
    "        self._registered_variables.remove(variable)\n",
    "\n",
    "\n",
    "    def _get_multisynaptic_indices(self):\n",
    "        template_kwds = {'multisynaptic_index': self.multisynaptic_index}\n",
    "        if self.multisynaptic_index is not None:\n",
    "            needed_variables = [self.multisynaptic_index]\n",
    "        else:\n",
    "            needed_variables=[]\n",
    "        return template_kwds, needed_variables\n",
    "\n",
    "    def _add_synapses_from_arrays(self, sources, targets, n, p,\n",
    "                                  namespace=None):\n",
    "        template_kwds, needed_variables = self._get_multisynaptic_indices()\n",
    "\n",
    "        variables = Variables(self)\n",
    "\n",
    "        sources = np.atleast_1d(sources).astype(np.int32)\n",
    "        targets = np.atleast_1d(targets).astype(np.int32)\n",
    "\n",
    "        # Check whether the values in sources/targets make sense\n",
    "        error_message = ('The given {source_or_target} indices contain '\n",
    "                         'values outside of the range [0, {max_value}] '\n",
    "                         'allowed for the {source_or_target} group '\n",
    "                         '\"{group_name}\"')\n",
    "        for indices, source_or_target, group in [(sources, 'source', self.source),\n",
    "                                                 (targets, 'target', self.target)]:\n",
    "            if np.max(indices) >= len(group) or np.min(indices) < 0:\n",
    "                raise IndexError(error_message.format(source_or_target=source_or_target,\n",
    "                                                      max_value=len(group)-1,\n",
    "                                                      group_name=group.name))\n",
    "        n = np.atleast_1d(n)\n",
    "        p = np.atleast_1d(p)\n",
    "\n",
    "        if not len(p) == 1 or p != 1:\n",
    "            use_connections = np.random.rand(len(sources)) < p\n",
    "            sources = sources[use_connections]\n",
    "            targets = targets[use_connections]\n",
    "            n = n[use_connections]\n",
    "        sources = sources.repeat(n)\n",
    "        targets = targets.repeat(n)\n",
    "\n",
    "        variables.add_array('sources', len(sources), dtype=np.int32,\n",
    "                            values=sources)\n",
    "        variables.add_array('targets', len(targets), dtype=np.int32,\n",
    "                            values=targets)\n",
    "        # These definitions are important to get the types right in C++\n",
    "        variables.add_auxiliary_variable('_real_sources', dtype=np.int32)\n",
    "        variables.add_auxiliary_variable('_real_targets', dtype=np.int32)\n",
    "        abstract_code = ''\n",
    "        if '_offset' in self.source.variables:\n",
    "            variables.add_reference('_source_offset', self.source, '_offset')\n",
    "            abstract_code += '_real_sources = sources + _source_offset\\n'\n",
    "        else:\n",
    "            abstract_code += '_real_sources = sources\\n'\n",
    "        if '_offset' in self.target.variables:\n",
    "            variables.add_reference('_target_offset', self.target, '_offset')\n",
    "            abstract_code += '_real_targets = targets + _target_offset\\n'\n",
    "        else:\n",
    "            abstract_code += '_real_targets = targets'\n",
    "        logger.debug(\"Creating synapses from group '%s' to group '%s', \"\n",
    "                     \"using pre-defined arrays)\" % (self.source.name,\n",
    "                                                    self.target.name))\n",
    "\n",
    "        codeobj = create_runner_codeobj(self,\n",
    "                                        abstract_code,\n",
    "                                        'synapses_create_array',\n",
    "                                        additional_variables=variables,\n",
    "                                        template_kwds=template_kwds,\n",
    "                                        needed_variables=needed_variables,\n",
    "                                        check_units=False,\n",
    "                                        run_namespace={})\n",
    "        codeobj()\n",
    "\n",
    "    def _expression_index_dependence(self, expr, namespace,\n",
    "                                     additional_indices=None):\n",
    "        '''\n",
    "        Returns the set of synaptic indices that expr depends on\n",
    "        '''\n",
    "        nr = NodeRenderer()\n",
    "        expr = nr.render_expr(expr)\n",
    "        deps = set()\n",
    "        if additional_indices is None:\n",
    "            additional_indices = {}\n",
    "        identifiers = get_identifiers_recursively([expr], self.variables)\n",
    "        variables = self.resolve_all({name for name in identifiers\n",
    "                                      if not name in additional_indices},\n",
    "                                     namespace)\n",
    "        if any(getattr(var, 'auto_vectorise', False) for var in variables.values()):\n",
    "            identifiers.add('_vectorisation_idx')\n",
    "\n",
    "        for varname in identifiers:\n",
    "            # Special handling of i and j -- they do not actually use pre-/\n",
    "            # postsynaptic indices (except for subgroups), they *are* the\n",
    "            # pre-/postsynaptic indices\n",
    "            if varname == 'i':\n",
    "                deps.add('_presynaptic_idx')\n",
    "            elif varname == 'j':\n",
    "                deps.add('_iterator_idx')\n",
    "            elif varname in additional_indices:\n",
    "                deps.add(additional_indices[varname])\n",
    "            else:\n",
    "                deps.add(self.variables.indices[varname])\n",
    "        if '0' in deps:\n",
    "            deps.remove('0')\n",
    "        return deps\n",
    "\n",
    "    def _add_synapses_generator(self, j, n, skip_if_invalid=False, namespace=None, level=0):\n",
    "        # Get the local namespace\n",
    "        if namespace is None:\n",
    "            namespace = get_local_namespace(level=level+1)\n",
    "\n",
    "        parsed = parse_synapse_generator(j)\n",
    "        self._check_parsed_synapses_generator(parsed, namespace)\n",
    "\n",
    "        # Referring to N_incoming/N_outgoing in the connect statement is\n",
    "        # ill-defined (see github issue #1227)\n",
    "        identifiers = get_identifiers_recursively([j], self.variables)\n",
    "        for var in ['N_incoming', 'N_outgoing']:\n",
    "            if var in identifiers:\n",
    "                raise ValueError(f'The connect statement cannot refer to '\n",
    "                                 f'\\'{var}\\'.')\n",
    "\n",
    "        template_kwds, needed_variables = self._get_multisynaptic_indices()\n",
    "        template_kwds.update(parsed)\n",
    "        template_kwds['skip_if_invalid'] = skip_if_invalid\n",
    "\n",
    "        if (parsed['iterator_func'] == 'sample' and\n",
    "                    parsed['iterator_kwds']['sample_size']=='fixed'):\n",
    "            raise NotImplementedError(\"Fixed sample size not implemented yet.\")\n",
    "\n",
    "        abstract_code = {'setup_iterator': '',\n",
    "                         'create_j': '',\n",
    "                         'create_cond': '',\n",
    "                         'update_post': ''}\n",
    "\n",
    "        additional_indices = {parsed['iteration_variable']: '_iterator_idx'}\n",
    "\n",
    "        setupiter = ''\n",
    "        for k, v in parsed['iterator_kwds'].items():\n",
    "            if v is not None and k!='sample_size':\n",
    "                deps = self._expression_index_dependence(v, namespace=namespace,\n",
    "                                                         additional_indices=additional_indices)\n",
    "                if '_postsynaptic_idx' in deps or '_iterator_idx' in deps:\n",
    "                    raise ValueError('Expression \"{}\" depends on postsynaptic '\n",
    "                                     'index or iterator'.format(v))\n",
    "                setupiter += '_iter_'+k+' = '+v+'\\n'\n",
    "\n",
    "        # rand() in the if condition depends on _vectorisation_idx, but not if\n",
    "        # its in the range expression (handled above)\n",
    "        additional_indices['_vectorisation_idx'] = '_iterator_idx'\n",
    "\n",
    "        postsynaptic_condition = False\n",
    "        postsynaptic_variable_used = False\n",
    "        if parsed['if_expression'] is not None:\n",
    "            deps = self._expression_index_dependence(parsed['if_expression'],\n",
    "                                                     namespace=namespace,\n",
    "                                                     additional_indices=additional_indices)\n",
    "            if '_postsynaptic_idx' in deps:\n",
    "                postsynaptic_condition = True\n",
    "                postsynaptic_variable_used = True\n",
    "            elif '_iterator_idx' in deps:\n",
    "                postsynaptic_condition = True\n",
    "        template_kwds['postsynaptic_condition'] = postsynaptic_condition\n",
    "        template_kwds['postsynaptic_variable_used'] = postsynaptic_variable_used\n",
    "\n",
    "        abstract_code['setup_iterator'] += setupiter\n",
    "        abstract_code['create_j'] += '_pre_idx = _raw_pre_idx \\n'\n",
    "        abstract_code['create_j'] += '_j = '+parsed['element']+'\\n'\n",
    "        if postsynaptic_condition:\n",
    "            abstract_code['create_cond'] += '_post_idx = _raw_post_idx \\n'\n",
    "        if parsed['if_expression'] is not None:\n",
    "            abstract_code['create_cond'] += ('_cond = ' +\n",
    "                                             parsed['if_expression'] + '\\n')\n",
    "            abstract_code['update_post'] += '_post_idx = _raw_post_idx \\n'\n",
    "        abstract_code['update_post'] += '_n = ' + str(n) + '\\n'\n",
    "\n",
    "        # This overwrites 'i' and 'j' in the synapses' variables dictionary\n",
    "        # This is necessary because in the context of synapse creation, i\n",
    "        # and j do not correspond to the sources/targets of the existing\n",
    "        # synapses but to all the possible sources/targets\n",
    "        variables = Variables(None)\n",
    "        # Will be set in the template\n",
    "        variables.add_auxiliary_variable('_i', dtype=np.int32)\n",
    "        variables.add_auxiliary_variable('_j', dtype=np.int32)\n",
    "        variables.add_auxiliary_variable('_iter_low', dtype=np.int32)\n",
    "        variables.add_auxiliary_variable('_iter_high', dtype=np.int32)\n",
    "        variables.add_auxiliary_variable('_iter_step', dtype=np.int32)\n",
    "        variables.add_auxiliary_variable('_iter_p')\n",
    "        variables.add_auxiliary_variable('_iter_size', dtype=np.int32)\n",
    "        variables.add_auxiliary_variable(parsed['iteration_variable'],\n",
    "                                         dtype=np.int32)\n",
    "        # Make sure that variables have the correct type in the code\n",
    "        variables.add_auxiliary_variable('_pre_idx', dtype=np.int32)\n",
    "        variables.add_auxiliary_variable('_post_idx', dtype=np.int32)\n",
    "        if parsed['if_expression'] is not None:\n",
    "            variables.add_auxiliary_variable('_cond', dtype=np.bool)\n",
    "        variables.add_auxiliary_variable('_n', dtype=np.int32)\n",
    "\n",
    "        if '_offset' in self.source.variables:\n",
    "            variables.add_reference('_source_offset', self.source, '_offset')\n",
    "        else:\n",
    "            variables.add_constant('_source_offset', value=0)\n",
    "\n",
    "        if '_offset' in self.target.variables:\n",
    "            variables.add_reference('_target_offset', self.target, '_offset')\n",
    "        else:\n",
    "            variables.add_constant('_target_offset', value=0)\n",
    "\n",
    "        variables.add_auxiliary_variable('_raw_pre_idx', dtype=np.int32)\n",
    "        variables.add_auxiliary_variable('_raw_post_idx', dtype=np.int32)\n",
    "\n",
    "        variable_indices = defaultdict(lambda: '_idx')\n",
    "        for varname in self.variables:\n",
    "            if self.variables.indices[varname] == '_presynaptic_idx':\n",
    "                variable_indices[varname] = '_raw_pre_idx'\n",
    "            elif self.variables.indices[varname] == '_postsynaptic_idx':\n",
    "                variable_indices[varname] = '_raw_post_idx'\n",
    "\n",
    "        if self.variables['i'] is self.variables['_synaptic_pre']:\n",
    "            variables.add_subexpression('i', '_i',\n",
    "                                        dtype=self.variables['i'].dtype)\n",
    "        if self.variables['j'] is self.variables['_synaptic_post']:\n",
    "            variables.add_subexpression('j', '_j',\n",
    "                                        dtype=self.variables['j'].dtype)\n",
    "\n",
    "        logger.debug((\"Creating synapses from group '%s' to group '%s', \"\n",
    "                      \"using generator '%s'\") % (self.source.name,\n",
    "                                                 self.target.name,\n",
    "                                                 parsed['original_expression']))\n",
    "\n",
    "        codeobj = create_runner_codeobj(self,\n",
    "                                        abstract_code,\n",
    "                                        'synapses_create_generator',\n",
    "                                        variable_indices=variable_indices,\n",
    "                                        additional_variables=variables,\n",
    "                                        template_kwds=template_kwds,\n",
    "                                        needed_variables=needed_variables,\n",
    "                                        check_units=False,\n",
    "                                        run_namespace=namespace)\n",
    "        codeobj()\n",
    "\n",
    "    def _check_parsed_synapses_generator(self, parsed, namespace):\n",
    "        \"\"\"\n",
    "        Type-check the parsed synapses generator. This function will raise a\n",
    "        TypeError if any of the arguments to the iterator function are of an\n",
    "        invalid type.\n",
    "        \"\"\"\n",
    "        if parsed['iterator_func'] == 'range':\n",
    "            # We expect all arguments of the range function to be integers\n",
    "            for argname, arg in parsed['iterator_kwds'].items():\n",
    "                identifiers = get_identifiers(arg)\n",
    "                variables = self.resolve_all(identifiers, run_namespace=namespace,\n",
    "                                             user_identifiers=identifiers)\n",
    "                annotated = brian_ast(arg, variables)\n",
    "                if annotated.dtype != 'integer':\n",
    "                    raise TypeError('The \"%s\" argument of the range function was '\n",
    "                                      '\"%s\", but it needs to be an '\n",
    "                                      'integer.' % (argname, arg))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
